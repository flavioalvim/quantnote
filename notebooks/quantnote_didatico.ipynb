{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuantNote - Tutorial Did√°tico\n",
    "\n",
    "## Sistema Quantitativo para Probabilidades de Retorno Condicionadas por Regime\n",
    "\n",
    "Este notebook demonstra passo a passo o funcionamento do sistema QuantNote.\n",
    "\n",
    "### Objetivo\n",
    "Calcular a probabilidade de um ativo atingir um retorno alvo em H per√≠odos, condicionada ao regime de mercado atual.\n",
    "\n",
    "### Conceitos Principais\n",
    "1. **Log-Retornos**: Usamos log(P_t/P_{t-1}) pois s√£o aditivos e sim√©tricos\n",
    "2. **Slope (Inclina√ß√£o)**: Tend√™ncia calculada via regress√£o linear do log-pre√ßo\n",
    "3. **Volatilidade**: Desvio padr√£o dos retornos em janela m√≥vel\n",
    "4. **Regimes**: Estados do mercado (bull/bear/flat combinados com alta/baixa volatilidade)\n",
    "5. **K-Means**: Clustering para detectar regimes automaticamente\n",
    "6. **Walk-Forward**: Valida√ß√£o para evitar overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Setup e Imports\n",
    "\n",
    "Primeiro, configuramos o ambiente e importamos os m√≥dulos necess√°rios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup do path para imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Imports padr√£o\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verificar se os imports funcionam\n",
    "print(\"Python path configurado!\")\n",
    "print(f\"Diret√≥rio de trabalho: {os.getcwd()}\")\n",
    "\n",
    "# Configura√ß√£o de visualiza√ß√£o\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2: Configura√ß√£o do Sistema\n",
    "\n",
    "O sistema usa Pydantic para validar configura√ß√µes. Isso garante que par√¢metros inv√°lidos sejam rejeitados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from config.settings import AnalysisConfig, InfrastructureConfig, Config\nfrom config.search_space import GAConfig, GASearchSpace\n\n# =============================================================================\n# CONFIGURA√á√ÉO CENTRALIZADA\n# =============================================================================\n# Altere os par√¢metros abaixo conforme necess√°rio.\n# Todos os notebooks usar√£o estas configura√ß√µes.\n\n# --- Ticker a analisar ---\nTICKER = \"BOVA11.SA\"\n\n# --- Par√¢metros de Predi√ß√£o (usados pelo GA) ---\nTARGET_RETURN = 0.03   # Target: 3% de varia√ß√£o\nHORIZON = 7            # Horizonte: 7 dias\n\n# --- Par√¢metros do GA Principal ---\nGA_POPULATION_SIZE = 100\nGA_GENERATIONS = 1000\nGA_N_FOLDS = 3\nGA_EARLY_STOPPING = True\n\n# --- Par√¢metros do GA para Multi-Target/Strike Grid (reduzido para demo) ---\nMULTI_GA_POPULATION_SIZE = 50\nMULTI_GA_GENERATIONS = 100\n\n# --- Par√¢metros de An√°lise Explorat√≥ria (Etapas 1-11) ---\nANALYSIS_WINDOW_SLOPE = 20\nANALYSIS_WINDOW_VOLATILITY = 20\nANALYSIS_WINDOW_ROLLING_RETURN = 20\nANALYSIS_WINDOW_TREND_INDICATOR = 10\nANALYSIS_TREND_SLOPE_MULTIPLIER = 2.0  # Janela do slope = window * multiplier\nANALYSIS_N_CLUSTERS = 6\nANALYSIS_TARGET_RETURN = 0.05  # 5% para explora√ß√£o did√°tica\n\n# --- Par√¢metros de M√©dias M√≥veis ---\nMA_FAST_PERIOD = 9     # M√©dia m√≥vel r√°pida\nMA_SLOW_PERIOD = 21    # M√©dia m√≥vel lenta\n\n# =============================================================================\n# Criar objetos de configura√ß√£o\n# =============================================================================\n\n# Config para an√°lise explorat√≥ria (etapas iniciais do notebook)\nconfig = Config()\nconfig.analysis.future_return_periods = HORIZON\nconfig.analysis.window_slope = ANALYSIS_WINDOW_SLOPE\nconfig.analysis.window_volatility = ANALYSIS_WINDOW_VOLATILITY\nconfig.analysis.window_rolling_return = ANALYSIS_WINDOW_ROLLING_RETURN\nconfig.analysis.window_trend_indicator = ANALYSIS_WINDOW_TREND_INDICATOR\nconfig.analysis.trend_slope_multiplier = ANALYSIS_TREND_SLOPE_MULTIPLIER\nconfig.analysis.n_clusters = ANALYSIS_N_CLUSTERS\nconfig.analysis.target_return = ANALYSIS_TARGET_RETURN\nconfig.analysis.ma_fast_period = MA_FAST_PERIOD\nconfig.analysis.ma_slow_period = MA_SLOW_PERIOD\n\n# Config do GA para otimiza√ß√£o principal\nga_config = GAConfig(\n    target_return=TARGET_RETURN,\n    horizon=HORIZON,\n    population_size=GA_POPULATION_SIZE,\n    generations=GA_GENERATIONS,\n    n_folds=GA_N_FOLDS,\n    stability_penalty=0.1,\n    elite_size=2,\n    early_stopping=GA_EARLY_STOPPING,\n)\n\n# Config do GA para multi-target e strike grid (reduzido)\nmulti_ga_config = GAConfig(\n    target_return=TARGET_RETURN,  # Ser√° sobrescrito pelo optimizer\n    horizon=HORIZON,\n    population_size=MULTI_GA_POPULATION_SIZE,\n    generations=MULTI_GA_GENERATIONS,\n    n_folds=GA_N_FOLDS,\n    early_stopping=GA_EARLY_STOPPING,\n)\n\n# =============================================================================\n# Exibir configura√ß√µes\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"CONFIGURA√á√ÉO CENTRALIZADA\")\nprint(\"=\" * 60)\n\nprint(f\"\\nüìä TICKER: {TICKER}\")\n\nprint(f\"\\nüéØ Par√¢metros de Predi√ß√£o:\")\nprint(f\"   Target Return: {TARGET_RETURN:.1%}\")\nprint(f\"   Horizonte: {HORIZON} dias\")\n\nprint(f\"\\nüß¨ GA Principal:\")\nprint(f\"   Popula√ß√£o: {GA_POPULATION_SIZE}\")\nprint(f\"   Gera√ß√µes: {GA_GENERATIONS}\")\nprint(f\"   Folds: {GA_N_FOLDS}\")\nprint(f\"   Early Stopping: {GA_EARLY_STOPPING}\")\n\nprint(f\"\\nüéØ GA Multi-Target/Strike Grid:\")\nprint(f\"   Popula√ß√£o: {MULTI_GA_POPULATION_SIZE}\")\nprint(f\"   Gera√ß√µes: {MULTI_GA_GENERATIONS}\")\n\nprint(f\"\\nüìà An√°lise Explorat√≥ria:\")\nprint(f\"   Window Slope: {ANALYSIS_WINDOW_SLOPE}\")\nprint(f\"   Window Volatility: {ANALYSIS_WINDOW_VOLATILITY}\")\nprint(f\"   Window Trend Indicator: {ANALYSIS_WINDOW_TREND_INDICATOR}\")\nprint(f\"   Trend Slope Multiplier: {ANALYSIS_TREND_SLOPE_MULTIPLIER}\")\nprint(f\"   Clusters: {ANALYSIS_N_CLUSTERS}\")\nprint(f\"   Target (explorat√≥rio): {ANALYSIS_TARGET_RETURN:.1%}\")\n\nprint(f\"\\nüìâ M√©dias M√≥veis:\")\nprint(f\"   MA R√°pida: {MA_FAST_PERIOD} per√≠odos\")\nprint(f\"   MA Lenta: {MA_SLOW_PERIOD} per√≠odos\")\n\nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3: Obten√ß√£o de Dados\n",
    "\n",
    "Usamos `YahooDataSource` para baixar dados OHLCV. O sistema tem rate limiting para evitar bloqueio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.infrastructure.yahoo_data_source import YahooDataSource\n",
    "from src.infrastructure.parquet_repository import ParquetRepository\n",
    "from src.infrastructure.file_logger import FileLogger, NullLogger\n",
    "\n",
    "# Criar logger (usar NullLogger para menos output)\n",
    "logger = NullLogger()  # ou FileLogger(\"quantnote\") para logs detalhados\n",
    "\n",
    "# Data source e repository\n",
    "data_source = YahooDataSource(calls_per_minute=5, logger=logger)\n",
    "repository = ParquetRepository(data_dir=\"../data\", logger=logger)\n",
    "\n",
    "# Usar ticker da configura√ß√£o centralizada\n",
    "ticker = TICKER\n",
    "\n",
    "print(f\"Buscando dados para {ticker}...\")\n",
    "\n",
    "# Verificar se dados no cache est√£o atualizados\n",
    "is_current, last_date = repository.is_data_current(ticker, max_age_days=1)\n",
    "\n",
    "if is_current:\n",
    "    print(f\"‚úÖ Cache atualizado (√∫ltima data: {last_date.date()})\")\n",
    "    df = repository.load(ticker)\n",
    "else:\n",
    "    if last_date:\n",
    "        print(f\"‚ö†Ô∏è  Cache desatualizado (√∫ltima data: {last_date.date()})\")\n",
    "    else:\n",
    "        print(\"üì• Dados n√£o encontrados no cache.\")\n",
    "    \n",
    "    print(\"Baixando dados atualizados do Yahoo Finance...\")\n",
    "    df = data_source.fetch_ohlcv(ticker)\n",
    "    \n",
    "    # Limpar arquivos antigos e salvar novos dados\n",
    "    deleted = repository.delete_old_files(ticker, keep_latest=0)\n",
    "    if deleted:\n",
    "        print(f\"üóëÔ∏è  {deleted} arquivo(s) antigo(s) removido(s)\")\n",
    "    \n",
    "    repository.save(df, ticker)\n",
    "    print(\"‚úÖ Dados atualizados salvos no cache.\")\n",
    "\n",
    "print(f\"\\n=== Dados Obtidos ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Per√≠odo: {df['date'].min().date()} a {df['date'].max().date()}\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4: Valida√ß√£o de Dados\n",
    "\n",
    "Antes de processar, validamos os dados para garantir qualidade.\n",
    "\n",
    "O sistema usa o padr√£o **Composite** para combinar m√∫ltiplos validadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.infrastructure.validators import create_default_validator\n",
    "\n",
    "# Criar validador composto\n",
    "validator = create_default_validator(\n",
    "    min_length=config.analysis.min_data_points,\n",
    "    max_window=max(\n",
    "        config.analysis.window_slope, \n",
    "        config.analysis.window_volatility,\n",
    "        config.analysis.window_trend_indicator\n",
    "    )\n",
    ")\n",
    "\n",
    "# Executar valida√ß√£o\n",
    "validation = validator.validate(df)\n",
    "\n",
    "print(\"=== Resultado da Valida√ß√£o ===\")\n",
    "print(f\"V√°lido: {'SIM' if validation.is_valid else 'N√ÉO'}\")\n",
    "\n",
    "if validation.errors:\n",
    "    print(f\"\\nERROS:\")\n",
    "    for error in validation.errors:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "if validation.warnings:\n",
    "    print(f\"\\nAVISOS:\")\n",
    "    for warning in validation.warnings:\n",
    "        print(f\"  - {warning}\")\n",
    "\n",
    "if validation.is_valid and not validation.warnings:\n",
    "    print(\"Todos os testes passaram sem avisos!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5: Calculadores Individuais\n",
    "\n",
    "Vamos explorar cada calculador individualmente para entender o que faz.\n",
    "\n",
    "Cada calculador implementa `IColumnCalculator` e segue o princ√≠pio de **Single Responsibility**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calculators.log_price_calculator import LogPriceCalculator\n",
    "\n",
    "# 5.1 - Log Price Calculator\n",
    "log_price_calc = LogPriceCalculator()\n",
    "\n",
    "print(\"=== LogPriceCalculator ===\")\n",
    "print(f\"Nome: {log_price_calc.name}\")\n",
    "print(f\"Colunas requeridas: {log_price_calc.required_columns}\")\n",
    "print(f\"Colunas produzidas: {log_price_calc.output_columns}\")\n",
    "\n",
    "# Aplicar\n",
    "df_step1 = log_price_calc.calculate(df)\n",
    "\n",
    "# Visualizar resultado\n",
    "print(f\"\\nFormula: log_close = ln(close)\")\n",
    "print(f\"\\nExemplo:\")\n",
    "print(df_step1[['date', 'close', 'log_close']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calculators.log_return_calculator import LogReturnCalculator\n",
    "\n",
    "# 5.2 - Log Return Calculator\n",
    "log_return_calc = LogReturnCalculator(window=config.analysis.window_rolling_return)\n",
    "\n",
    "print(\"=== LogReturnCalculator ===\")\n",
    "print(f\"Nome: {log_return_calc.name}\")\n",
    "print(f\"Colunas requeridas: {log_return_calc.required_columns}\")\n",
    "print(f\"Colunas produzidas: {log_return_calc.output_columns}\")\n",
    "\n",
    "# Aplicar\n",
    "df_step2 = log_return_calc.calculate(df_step1)\n",
    "\n",
    "print(f\"\\nFormulas:\")\n",
    "print(f\"  log_return = ln(close_t / close_{{t-1}})\")\n",
    "print(f\"  log_return_rolling_20 = sum(log_return over 20 days)\")\n",
    "print(f\"\\nExemplo:\")\n",
    "print(df_step2[['date', 'close', 'log_return', f'log_return_rolling_{config.analysis.window_rolling_return}']].head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calculators.volatility_calculator import VolatilityCalculator\n",
    "\n",
    "# 5.3 - Volatility Calculator\n",
    "vol_calc = VolatilityCalculator(window=config.analysis.window_volatility)\n",
    "\n",
    "print(\"=== VolatilityCalculator ===\")\n",
    "print(f\"Nome: {vol_calc.name}\")\n",
    "print(f\"Colunas requeridas: {vol_calc.required_columns}\")\n",
    "print(f\"Colunas produzidas: {vol_calc.output_columns}\")\n",
    "\n",
    "# Aplicar\n",
    "df_step3 = vol_calc.calculate(df_step2)\n",
    "\n",
    "print(f\"\\nFormula: volatility = std(log_return over {config.analysis.window_volatility} days)\")\n",
    "print(f\"\\nExemplo:\")\n",
    "print(df_step3[['date', 'log_return', f'volatility_{config.analysis.window_volatility}']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calculators.slope_calculator import SlopeCalculator\n",
    "\n",
    "# 5.4 - Slope Calculator\n",
    "slope_calc = SlopeCalculator(window=config.analysis.window_slope)\n",
    "\n",
    "print(\"=== SlopeCalculator ===\")\n",
    "print(f\"Nome: {slope_calc.name}\")\n",
    "print(f\"Colunas requeridas: {slope_calc.required_columns}\")\n",
    "print(f\"Colunas produzidas: {slope_calc.output_columns}\")\n",
    "\n",
    "# Aplicar\n",
    "df_step4 = slope_calc.calculate(df_step3)\n",
    "\n",
    "print(f\"\\nFormula: slope = coef angular da regress√£o linear de log_close sobre {config.analysis.window_slope} dias\")\n",
    "print(f\"\\nInterpreta√ß√£o:\")\n",
    "print(f\"  slope > 0: tend√™ncia de alta\")\n",
    "print(f\"  slope < 0: tend√™ncia de baixa\")\n",
    "print(f\"  slope ‚âà 0: mercado lateral\")\n",
    "print(f\"\\nExemplo:\")\n",
    "print(df_step4[['date', 'log_close', f'slope_{config.analysis.window_slope}']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calculators.ma_distance_calculator import MADistanceCalculator\n",
    "\n",
    "# 5.5 - MA Distance Calculator\n",
    "ma_dist_calc = MADistanceCalculator(\n",
    "    fast_period=config.analysis.ma_fast_period,\n",
    "    slow_period=config.analysis.ma_slow_period\n",
    ")\n",
    "\n",
    "print(\"=== MADistanceCalculator ===\")\n",
    "print(f\"Nome: {ma_dist_calc.name}\")\n",
    "print(f\"Colunas requeridas: {ma_dist_calc.required_columns}\")\n",
    "print(f\"Colunas produzidas: {ma_dist_calc.output_columns}\")\n",
    "\n",
    "# Aplicar\n",
    "df_step4b = ma_dist_calc.calculate(df_step4)\n",
    "\n",
    "ma_col = f'ma_dist_{config.analysis.ma_fast_period}_{config.analysis.ma_slow_period}'\n",
    "\n",
    "print(f\"\\nFormula:\")\n",
    "print(f\"  MA_fast = SMA(close, {config.analysis.ma_fast_period})\")\n",
    "print(f\"  MA_slow = SMA(close, {config.analysis.ma_slow_period})\")\n",
    "print(f\"  distance = MA_fast - MA_slow\")\n",
    "print(f\"  {ma_col} = normalize(distance, min=-1, max=1)\")\n",
    "\n",
    "print(f\"\\nInterpreta√ß√£o:\")\n",
    "print(f\"  > 0: tend√™ncia de alta (MA r√°pida > MA lenta)\")\n",
    "print(f\"  < 0: tend√™ncia de baixa (MA r√°pida < MA lenta)\")\n",
    "print(f\"  ‚âà 0: m√©dias cruzando/convergindo\")\n",
    "\n",
    "print(f\"\\nExemplo:\")\n",
    "print(df_step4b[['date', 'close', ma_col]].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.calculators.trend_indicator_calculator import TrendIndicatorCalculator\n\n# 5.5b - Trend Indicator Calculator\n# Indicador de tend√™ncia baseado em Higher Lows e Lower Highs, filtrado pela dire√ß√£o do slope\ntrend_calc = TrendIndicatorCalculator(\n    window=config.analysis.window_trend_indicator,\n    slope_multiplier=config.analysis.trend_slope_multiplier\n)\n\nprint(\"=== TrendIndicatorCalculator ===\")\nprint(f\"Nome: {trend_calc.name}\")\nprint(f\"Colunas requeridas: {trend_calc.required_columns}\")\nprint(f\"Colunas produzidas: {trend_calc.output_columns}\")\n\n# Aplicar\ndf_step4c = trend_calc.calculate(df_step4b)\n\nwindow = config.analysis.window_trend_indicator\nslope_window = int(window * config.analysis.trend_slope_multiplier)\n\nprint(f\"\\n=== L√≥gica ===\")\nprint(f\"1. Calcula slope em janela de {slope_window} dias (window * multiplier)\")\nprint(f\"2. Se slope > 0 (uptrend): conta higher_lows na janela de {window} dias ‚Üí valor POSITIVO\")\nprint(f\"3. Se slope <= 0 (downtrend): conta lower_highs na janela de {window} dias ‚Üí valor NEGATIVO\")\n\nprint(f\"\\n=== Colunas de Sa√≠da ===\")\nprint(f\"- trend_indicator_{window}: Valor COM sinal (+higher_lows ou -lower_highs)\")\nprint(f\"- trend_indicator_norm_{window}: Normalizado entre -1 e +1\")\nprint(f\"- trend_strength_{window}: Valor absoluto (for√ßa sem dire√ß√£o)\")\nprint(f\"- trend_strength_norm_{window}: For√ßa normalizada entre 0 e 1\")\n\nprint(f\"\\n=== Uso ===\")\nprint(f\"- Para visualiza√ß√£o/interpreta√ß√£o: use trend_indicator (com sinal)\")\nprint(f\"- Para K-Means clustering: use trend_strength (abs) para evitar redund√¢ncia com slope\")\nprint(f\"  (O sinal j√° est√° capturado pelo slope, ent√£o usamos apenas a magnitude)\")\n\nprint(f\"\\n=== Amostra (valores com sinal) ===\")\ntrend_cols = [f'trend_indicator_{window}', f'trend_indicator_norm_{window}']\nprint(df_step4c[['date', 'high', 'low'] + trend_cols].dropna().tail(10))\n\nprint(f\"\\n=== Amostra (for√ßa absoluta - usada no K-Means) ===\")\nstrength_cols = [f'trend_strength_{window}', f'trend_strength_norm_{window}']\nprint(df_step4c[['date'] + strength_cols].dropna().tail(10))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calculators.future_return_calculator import FutureReturnCalculator\n",
    "\n",
    "# 5.6 - Future Return Calculator\n",
    "future_calc = FutureReturnCalculator(horizon=config.analysis.future_return_periods)\n",
    "\n",
    "print(\"=== FutureReturnCalculator ===\")\n",
    "print(f\"Nome: {future_calc.name}\")\n",
    "print(f\"Colunas requeridas: {future_calc.required_columns}\")\n",
    "print(f\"Colunas produzidas: {future_calc.output_columns}\")\n",
    "\n",
    "# Aplicar\n",
    "df_step5 = future_calc.calculate(df_step4)\n",
    "\n",
    "print(f\"\\nFormula: log_return_future_{config.analysis.future_return_periods} = ln(close_{{t+{config.analysis.future_return_periods}}} / close_t)\")\n",
    "print(f\"\\nNOTA: Esta coluna √© a vari√°vel TARGET que queremos prever!\")\n",
    "print(f\"\\nExemplo:\")\n",
    "print(df_step5[['date', 'close', f'log_return_future_{config.analysis.future_return_periods}']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 6: Pipeline com Resolu√ß√£o Autom√°tica de Depend√™ncias\n",
    "\n",
    "O `CalculatorPipeline` usa **topological sort** para ordenar os calculadores automaticamente.\n",
    "\n",
    "Isso implementa o princ√≠pio **Open/Closed** - podemos adicionar calculadores sem modificar o pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.calculators.pipeline import CalculatorPipeline\nfrom src.calculators.log_price_calculator import LogPriceCalculator\nfrom src.calculators.log_return_calculator import LogReturnCalculator\nfrom src.calculators.future_return_calculator import FutureReturnCalculator\nfrom src.calculators.volatility_calculator import VolatilityCalculator\nfrom src.calculators.slope_calculator import SlopeCalculator\nfrom src.calculators.ma_distance_calculator import MADistanceCalculator\nfrom src.calculators.trend_indicator_calculator import TrendIndicatorCalculator\n\n# Criar pipeline (note que a ordem n√£o importa - ser√° resolvida automaticamente)\npipeline = CalculatorPipeline([\n    SlopeCalculator(window=config.analysis.window_slope),      # Depende de log_close\n    LogReturnCalculator(window=config.analysis.window_rolling_return),  # Depende de close\n    VolatilityCalculator(window=config.analysis.window_volatility),     # Depende de log_return\n    LogPriceCalculator(),                                       # Depende de close\n    FutureReturnCalculator(horizon=config.analysis.future_return_periods),  # Depende de close\n    MADistanceCalculator(                                       # Depende de close\n        fast_period=config.analysis.ma_fast_period,\n        slow_period=config.analysis.ma_slow_period\n    ),\n    TrendIndicatorCalculator(                                   # Depende de high, low, close\n        window=config.analysis.window_trend_indicator,\n        slope_multiplier=config.analysis.trend_slope_multiplier\n    )\n], logger=logger)\n\n# Executar pipeline\ndf_analysis = pipeline.run(df)\n\nprint(\"=== Pipeline Executado ===\")\nprint(f\"Ordem de execu√ß√£o: {pipeline.get_execution_order()}\")\nprint(f\"\\nColunas originais: {len(df.columns)}\")\nprint(f\"Colunas ap√≥s pipeline: {len(df_analysis.columns)}\")\nprint(f\"\\nNovas colunas: {list(pipeline.get_all_output_columns())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultado do pipeline\n",
    "print(\"=== Dados Ap√≥s Pipeline ===\")\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', 10)\n",
    "df_analysis.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 7: Classifica√ß√£o Manual de Regimes\n",
    "\n",
    "Uma abordagem √© usar thresholds manuais para classificar regimes baseado em slope e volatilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.regime_classifier import ManualRegimeClassifier\n",
    "\n",
    "# Nomes das colunas\n",
    "slope_col = f'slope_{config.analysis.window_slope}'\n",
    "vol_col = f'volatility_{config.analysis.window_volatility}'\n",
    "\n",
    "# Criar classificador manual (thresholds autom√°ticos)\n",
    "manual_classifier = ManualRegimeClassifier(\n",
    "    slope_column=slope_col,\n",
    "    volatility_column=vol_col\n",
    ")\n",
    "\n",
    "# Classificar\n",
    "df_manual = manual_classifier.classify(df_analysis)\n",
    "\n",
    "# Resultados\n",
    "print(\"=== Classifica√ß√£o Manual de Regimes ===\")\n",
    "print(f\"\\nThresholds usados: {manual_classifier.get_thresholds()}\")\n",
    "print(f\"\\nDistribui√ß√£o de regimes:\")\n",
    "print(df_manual['regime'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribui√ß√£o por regime\n",
    "regime_counts = df_manual['regime'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = {\n",
    "    'bull_high_vol': 'lightgreen',\n",
    "    'bull_low_vol': 'darkgreen',\n",
    "    'bear_high_vol': 'lightcoral',\n",
    "    'bear_low_vol': 'darkred',\n",
    "    'flat_high_vol': 'yellow',\n",
    "    'flat_low_vol': 'gold'\n",
    "}\n",
    "bar_colors = [colors.get(r, 'gray') for r in regime_counts.index]\n",
    "regime_counts.plot(kind='bar', ax=ax, color=bar_colors, edgecolor='black')\n",
    "ax.set_title('Distribui√ß√£o de Regimes (Classifica√ß√£o Manual)')\n",
    "ax.set_xlabel('Regime')\n",
    "ax.set_ylabel('Frequ√™ncia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 8: Classifica√ß√£o com K-Means\n",
    "\n",
    "K-Means detecta regimes automaticamente baseado em m√∫ltiplas features.\n",
    "\n",
    "Isso √© mais robusto que thresholds manuais pois considera todas as features simultaneamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.kmeans_regimes import KMeansRegimeClassifier\n",
    "\n",
    "# Criar classificador K-Means\n",
    "kmeans = KMeansRegimeClassifier(\n",
    "    n_clusters=config.analysis.n_clusters,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Fit e Predict\n",
    "df_kmeans = kmeans.fit_predict(df_analysis)\n",
    "\n",
    "print(\"=== Classifica√ß√£o K-Means ===\")\n",
    "print(f\"\\nFeatures usadas: {kmeans.feature_columns}\")\n",
    "print(f\"N√∫mero de clusters: {config.analysis.n_clusters}\")\n",
    "print(f\"\\nDistribui√ß√£o de clusters:\")\n",
    "print(df_kmeans['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas por cluster\n",
    "future_col = f'log_return_future_{config.analysis.future_return_periods}'\n",
    "stats = kmeans.compute_statistics(df_kmeans, future_col)\n",
    "interpretations = kmeans.interpret_clusters(df_kmeans, slope_col)\n",
    "\n",
    "print(\"=== Estat√≠sticas por Cluster ===\")\n",
    "for stat in stats:\n",
    "    interp = interpretations.get(stat.cluster_id, 'unknown')\n",
    "    print(f\"\\nCluster {stat.cluster_id} ({interp.upper()}):\")\n",
    "    print(f\"  Observa√ß√µes: {stat.count} ({stat.percentage:.1f}%)\")\n",
    "    print(f\"  Retorno futuro m√©dio: {stat.future_return_mean:.4f}\")\n",
    "    print(f\"  Desvio padr√£o: {stat.future_return_std:.4f}\")\n",
    "    print(f\"  Features m√©dias: {stat.feature_means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Etapa 8b: Explicando os Clusters\n\nUsamos Decision Tree e Random Forest para entender quais vari√°veis s√£o mais importantes para definir cada cluster.\n\nIsso ajuda a interpretar o que o K-Means \"aprendeu\" sobre os regimes de mercado.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from src.analysis import CompositeExplainer, DecisionTreeExplainer\n\n# Criar explainer composto (Decision Tree + Random Forest)\nexplainer = CompositeExplainer(\n    tree_max_depth=5,           # Profundidade da √°rvore (interpretabilidade)\n    forest_n_estimators=100,    # N√∫mero de √°rvores no Random Forest\n    min_samples_leaf=20         # M√≠nimo de amostras por folha\n)\n\n# Detectar features automaticamente ou usar as do kmeans\nfeature_cols = kmeans.feature_columns\nprint(f\"Features usadas: {feature_cols}\")\n\n# Fit no DataFrame com clusters\nexplainer.fit(df_kmeans, cluster_column='cluster', feature_columns=feature_cols)\n\n# Sum√°rio completo\nexplainer.print_summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Comparar import√¢ncias: Decision Tree vs Random Forest\nfig = explainer.plot_importances()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualizar a √°rvore de decis√£o\nfig = explainer.plot_tree(figsize=(20, 10), fontsize=9)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Top 3 features mais importantes\nprint(\"=== Top 3 Features ===\")\nfor feature, importance in explainer.get_top_features(3):\n    print(f\"  {feature}: {importance:.3f}\")\n\n# M√©tricas do modelo\nmetrics = explainer.get_metrics()\nprint(f\"\\n=== M√©tricas ===\")\nprint(f\"  Accuracy (train): {metrics.accuracy:.1%}\")\nprint(f\"  Accuracy (CV):    {metrics.cv_accuracy_mean:.1%} (+/- {metrics.cv_accuracy_std:.1%})\")\nprint(f\"  Samples:          {metrics.n_samples}\")\nprint(f\"  Clusters:         {metrics.n_clusters}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 9: C√°lculo de Probabilidades\n",
    "\n",
    "Agora calculamos a probabilidade de atingir o retorno alvo, condicionada por regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.probability_calculator import ProbabilityCalculator\n",
    "\n",
    "# Usando classifica√ß√£o manual\n",
    "prob_calc_manual = ProbabilityCalculator(\n",
    "    future_return_column=future_col,\n",
    "    target_return=config.analysis.target_return,\n",
    "    regime_column='regime'\n",
    ")\n",
    "\n",
    "# Gerar relat√≥rio completo\n",
    "report = prob_calc_manual.generate_report(df_manual)\n",
    "\n",
    "print(\"=== Relat√≥rio de Probabilidades (Regimes Manuais) ===\")\n",
    "print(f\"\\nRetorno alvo: {report['target_return']:.1%}\")\n",
    "print(f\"Log-retorno alvo: {report['log_target']:.4f}\")\n",
    "print(f\"\\nProbabilidade incondicional: {report['raw_probability_pct']:.2f}%\")\n",
    "\n",
    "print(f\"\\nProbabilidades condicionais:\")\n",
    "for regime, data in report['conditional_probabilities'].items():\n",
    "    print(f\"  {regime}:\")\n",
    "    print(f\"    P(hit) = {data['probability_pct']:.2f}%\")\n",
    "    print(f\"    n = {data['count']}\")\n",
    "    print(f\"    retorno m√©dio = {data['mean_return']:.4f}\")\n",
    "\n",
    "print(f\"\\nM√©tricas de separa√ß√£o:\")\n",
    "print(f\"  Delta P: {report['separation_metrics']['delta_p']:.4f}\")\n",
    "print(f\"  (diferen√ßa entre maior e menor probabilidade)\")\n",
    "print(f\"  Information Ratio: {report['separation_metrics']['information_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando classifica√ß√£o K-Means\n",
    "prob_calc_kmeans = ProbabilityCalculator(\n",
    "    future_return_column=future_col,\n",
    "    target_return=config.analysis.target_return,\n",
    "    regime_column='cluster'\n",
    ")\n",
    "\n",
    "report_kmeans = prob_calc_kmeans.generate_report(df_kmeans)\n",
    "\n",
    "print(\"=== Relat√≥rio de Probabilidades (K-Means) ===\")\n",
    "print(f\"\\nRetorno alvo: {report_kmeans['target_return']:.1%}\")\n",
    "print(f\"\\nProbabilidade incondicional: {report_kmeans['raw_probability_pct']:.2f}%\")\n",
    "\n",
    "print(f\"\\nProbabilidades por cluster:\")\n",
    "for cluster_id, data in sorted(report_kmeans['conditional_probabilities'].items()):\n",
    "    interp = interpretations.get(int(float(cluster_id)), 'unknown')\n",
    "    print(f\"  Cluster {cluster_id} ({interp}): P(hit) = {data['probability_pct']:.2f}% (n={data['count']})\")\n",
    "\n",
    "print(f\"\\nDelta P: {report_kmeans['separation_metrics']['delta_p']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 10: Visualiza√ß√µes\n",
    "\n",
    "Visualizamos a distribui√ß√£o de retornos por regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.histogram_plotter import HistogramPlotter, PriceRegimePlotter\n",
    "\n",
    "# Histograma geral\n",
    "hist_plotter = HistogramPlotter(\n",
    "    return_column=future_col,\n",
    "    regime_column='regime'\n",
    ")\n",
    "\n",
    "fig = hist_plotter.plot(df_manual)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas por regime\n",
    "fig = hist_plotter.plot_by_regime(df_manual, target_return=config.analysis.target_return)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre√ßo com background de regime\n",
    "df_manual_indexed = df_manual.set_index('date')\n",
    "\n",
    "price_plotter = PriceRegimePlotter(regime_column='regime')\n",
    "fig = price_plotter.plot(df_manual_indexed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 11: Walk-Forward Validation\n",
    "\n",
    "Para evitar overfitting, usamos walk-forward validation.\n",
    "\n",
    "Isso simula como o modelo performaria em tempo real, sempre treinando no passado e testando no futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.time_series_splitter import TimeSeriesSplitter\n",
    "\n",
    "# Criar splitter\n",
    "splitter = TimeSeriesSplitter(train_ratio=0.7)\n",
    "\n",
    "# Demonstrar walk-forward splits\n",
    "print(\"=== Walk-Forward Splits ===\")\n",
    "for split in splitter.walk_forward_split(df, n_folds=5, min_train_size=252):\n",
    "    train_start = split.train['date'].min().date()\n",
    "    train_end = split.train['date'].max().date()\n",
    "    test_start = split.test['date'].min().date()\n",
    "    test_end = split.test['date'].max().date()\n",
    "    \n",
    "    print(f\"\\nFold {split.fold}:\")\n",
    "    print(f\"  Train: {train_start} a {train_end} ({len(split.train)} obs)\")\n",
    "    print(f\"  Test:  {test_start} a {test_end} ({len(split.test)} obs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 12: Otimiza√ß√£o com Algoritmo Gen√©tico (Otimizado)\n",
    "\n",
    "O GA busca os melhores par√¢metros automaticamente.\n",
    "\n",
    "**Otimiza√ß√µes implementadas:**\n",
    "1. **Paraleliza√ß√£o**: Avalia√ß√£o de fitness usa m√∫ltiplos cores CPU\n",
    "2. **Cache**: Cromossomos id√™nticos n√£o s√£o reavaliados\n",
    "3. **Numba**: C√°lculo de slope ~10-50x mais r√°pido (ap√≥s warm-up)\n",
    "4. **KMeans otimizado**: Algoritmo Lloyd para melhor performance\n",
    "\n",
    "**NOTA sobre Numba**: A primeira execu√ß√£o ser√° mais lenta devido √† compila√ß√£o JIT. Execu√ß√µes subsequentes ser√£o significativamente mais r√°pidas. O cache do Numba persiste entre sess√µes.\n",
    "\n",
    "**NOTA**: Esta etapa √© computacionalmente intensiva. Reduzimos os par√¢metros para demonstra√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calculators.slope_calculator import SlopeCalculator\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Verificar otimiza√ß√µes dispon√≠veis\n",
    "print(\"=== Verifica√ß√£o de Otimiza√ß√µes ===\")\n",
    "print(f\"Numba dispon√≠vel para SlopeCalculator: {SlopeCalculator.is_numba_available()}\")\n",
    "print(f\"CPUs dispon√≠veis para paraleliza√ß√£o: {mp.cpu_count()}\")\n",
    "\n",
    "# ga_config j√° definido na c√©lula de configura√ß√£o centralizada\n",
    "print(\"\\n=== Configura√ß√£o do GA (da c√©lula centralizada) ===\")\n",
    "print(f\"Target Return: {ga_config.target_return:.1%}\")\n",
    "print(f\"Horizonte: {ga_config.horizon} dias\")\n",
    "print(f\"Popula√ß√£o: {ga_config.population_size}\")\n",
    "print(f\"Gera√ß√µes: {ga_config.generations}\")\n",
    "print(f\"Folds: {ga_config.n_folds}\")\n",
    "print(f\"Early Stopping: {ga_config.early_stopping}\")\n",
    "\n",
    "# Estimativa de tempo\n",
    "tempo_por_gen = ga_config.population_size * 0.035  # ~0.035s por cromossomo\n",
    "tempo_total = ga_config.generations * tempo_por_gen\n",
    "print(f\"\\nTempo estimado: {tempo_total/60:.1f} minutos\")\n",
    "print(\"\\nNOTA: Use Ctrl+C para interromper e salvar checkpoint a qualquer momento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.optimization.genetic_algorithm import GeneticAlgorithm, GACheckpoint\nfrom src.optimization.progress_callback import LiveProgressCallback\n\n# Criar callback de progresso visual\nprogress_callback = LiveProgressCallback(\n    total_generations=ga_config.generations,\n    population_size=ga_config.population_size,\n    update_every=1\n)\n\n# Executar GA\n# NOTA: parallel=False √© mais est√°vel em Jupyter notebooks\nga = GeneticAlgorithm(\n    df, \n    ga_config, \n    logger=logger,\n    progress_callback=progress_callback,\n    n_workers=None\n)\n\ntry:\n    result = ga.run(\n        verbose=False,\n        parallel=False,  # Desativado para evitar problemas no Jupyter\n        auto_checkpoint_path=\"ga_checkpoint.json\",\n        checkpoint_every=10\n    )\nexcept KeyboardInterrupt:\n    print(\"\\nInterrompido! Salvando checkpoint...\")\n    ga.save_checkpoint(\"ga_checkpoint.json\")\n    print(\"Checkpoint salvo em 'ga_checkpoint.json'\")\n    print(\"Para retomar: checkpoint = GACheckpoint.load('ga_checkpoint.json')\")\n    raise\n\nprogress_callback.finalize()\n\nprint(\"\\n=== Melhores Par√¢metros Encontrados ===\")\nbest = result.best_chromosome\nprint(f\"  Window Slope: {best.window_slope}\")\nprint(f\"  Window Volatility: {best.window_volatility}\")\nprint(f\"  Window Rolling Return: {best.window_rolling_return}\")\nprint(f\"  Window Trend Indicator: {best.window_trend_indicator}\")\nprint(f\"  Trend Slope Multiplier: {best.trend_slope_multiplier}\")\nprint(f\"  N Clusters: {best.n_clusters}\")\nprint(f\"  MA Fast Period: {best.ma_fast_period}\")\nprint(f\"  MA Slow Period: {best.ma_slow_period}\")\nprint(f\"  Use Volatility: {best.use_volatility}\")\nprint(f\"  Use Rolling Return: {best.use_rolling_return}\")\nprint(f\"  Use MA Distance: {best.use_ma_distance}\")\nprint(f\"  Use Trend Indicator: {best.use_trend_indicator}\")\n\nprint(f\"\\nPar√¢metros fixos (da configura√ß√£o):\")\nprint(f\"  Target Return: {ga_config.target_return:.2%}\")\nprint(f\"  Horizon: {ga_config.horizon} dias\")\n\nprint(f\"\\nM√©tricas:\")\nprint(f\"  Fitness: {result.best_fitness:.4f}\")\nprint(f\"  Delta P (test): {result.best_metrics.delta_p_test:.4f}\")\nprint(f\"  Overfitting Ratio: {result.best_metrics.overfitting_ratio:.2f}\")\n\nprint(f\"\\nEstat√≠sticas:\")\nprint(f\"  Total avalia√ß√µes: {result.all_evaluations}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar evolu√ß√£o do fitness\n",
    "generations = [h[0] for h in result.history]\n",
    "best_fitness = [h[1] for h in result.history]\n",
    "mean_fitness = [h[2] for h in result.history]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(generations, best_fitness, 'b-', label='Best Fitness', linewidth=2)\n",
    "ax.plot(generations, mean_fitness, 'g--', label='Mean Fitness', alpha=0.7)\n",
    "ax.set_xlabel('Generation')\n",
    "ax.set_ylabel('Fitness')\n",
    "ax.set_title('Evolu√ß√£o do Algoritmo Gen√©tico')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 13: Aplicar Melhores Par√¢metros\n",
    "\n",
    "Agora aplicamos os par√¢metros otimizados e recalculamos as probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.optimization.calculator_factory import CalculatorFactory\n",
    "\n",
    "# Factory para criar pipeline com os melhores par√¢metros\n",
    "# horizon vem da configura√ß√£o, n√£o do cromossomo\n",
    "factory = CalculatorFactory(horizon=ga_config.horizon)\n",
    "best_pipeline = factory.create_pipeline(best)\n",
    "feature_cols = factory.get_feature_columns(best)\n",
    "future_col_best = factory.get_future_return_column()\n",
    "\n",
    "# Processar dados\n",
    "df_best = best_pipeline.run(df)\n",
    "\n",
    "# Clustering\n",
    "best_kmeans = KMeansRegimeClassifier(\n",
    "    n_clusters=best.n_clusters,\n",
    "    feature_columns=feature_cols\n",
    ")\n",
    "df_best = best_kmeans.fit_predict(df_best)\n",
    "\n",
    "# Probabilidades (target_return vem da configura√ß√£o)\n",
    "best_prob = ProbabilityCalculator(\n",
    "    future_return_column=future_col_best,\n",
    "    target_return=ga_config.target_return,\n",
    "    regime_column='cluster'\n",
    ")\n",
    "\n",
    "best_report = best_prob.generate_report(df_best)\n",
    "\n",
    "print(\"=== Relat√≥rio com Par√¢metros Otimizados ===\")\n",
    "print(f\"\\nRetorno alvo: {ga_config.target_return:.1%}\")\n",
    "print(f\"Horizonte: {ga_config.horizon} dias\")\n",
    "print(f\"\\nProbabilidade incondicional: {best_report['raw_probability_pct']:.2f}%\")\n",
    "\n",
    "# Interpretar clusters\n",
    "slope_col_best = f'slope_{best.window_slope}'\n",
    "best_interp = best_kmeans.interpret_clusters(df_best, slope_col_best)\n",
    "\n",
    "print(f\"\\nProbabilidades por cluster:\")\n",
    "for cluster_id, data in sorted(best_report['conditional_probabilities'].items()):\n",
    "    interp = best_interp.get(int(float(cluster_id)), 'unknown')\n",
    "    print(f\"  Cluster {cluster_id} ({interp.upper()}): P(hit) = {data['probability_pct']:.2f}% (n={data['count']})\")\n",
    "\n",
    "print(f\"\\nDelta P: {best_report['separation_metrics']['delta_p']:.4f}\")\n",
    "print(f\"Information Ratio: {best_report['separation_metrics']['information_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Explicar clusters dos par√¢metros otimizados\nfrom src.analysis import CompositeExplainer\n\nbest_explainer = CompositeExplainer(\n    tree_max_depth=5,\n    forest_n_estimators=100,\n    min_samples_leaf=20\n)\n\n# Fit no DataFrame com clusters otimizados\nbest_explainer.fit(df_best, cluster_column='cluster', feature_columns=feature_cols)\n\n# Sum√°rio\nprint(\"=== Explica√ß√£o dos Clusters (Par√¢metros Otimizados) ===\")\nbest_explainer.print_summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualizar import√¢ncias e √°rvore (par√¢metros otimizados)\nfig = best_explainer.plot_importances()\nplt.suptitle('Feature Importances - Par√¢metros Otimizados', y=1.02)\nplt.show()\n\n# Visualizar √°rvore de decis√£o\nfig = best_explainer.plot_tree(figsize=(18, 8), fontsize=8)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 14: Visualiza√ß√µes com Par√¢metros Otimizados\n",
    "\n",
    "Agora repetimos todas as visualiza√ß√µes usando os par√¢metros do indiv√≠duo vencedor do GA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas por cluster com par√¢metros otimizados\n",
    "stats_best = best_kmeans.compute_statistics(df_best, future_col_best)\n",
    "\n",
    "print(\"=== Estat√≠sticas por Cluster (Par√¢metros Otimizados) ===\")\n",
    "for stat in stats_best:\n",
    "    interp = best_interp.get(stat.cluster_id, 'unknown')\n",
    "    print(f\"\\nCluster {stat.cluster_id} ({interp.upper()}):\")\n",
    "    print(f\"  Observa√ß√µes: {stat.count} ({stat.percentage:.1f}%)\")\n",
    "    print(f\"  Retorno futuro m√©dio: {stat.future_return_mean:.4f}\")\n",
    "    print(f\"  Desvio padr√£o: {stat.future_return_std:.4f}\")\n",
    "    print(f\"  Features m√©dias: {stat.feature_means}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de retornos com par√¢metros otimizados\n",
    "hist_plotter_best = HistogramPlotter(\n",
    "    return_column=future_col_best,\n",
    "    regime_column='cluster'\n",
    ")\n",
    "\n",
    "fig = hist_plotter_best.plot(df_best)\n",
    "plt.suptitle(f'Distribui√ß√£o de Retornos Futuros ({ga_config.horizon} dias) - Par√¢metros Otimizados', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas por cluster com linha do target\n",
    "fig = hist_plotter_best.plot_by_regime(df_best, target_return=ga_config.target_return)\n",
    "plt.suptitle(f'Distribui√ß√£o por Cluster - Target: {ga_config.target_return:.1%} em {ga_config.horizon} dias', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pre√ßo com background de cluster otimizado\ndf_best_indexed = df_best.set_index('date')\n\nprice_plotter_best = PriceRegimePlotter(regime_column='cluster')\nfig = price_plotter_best.plot(df_best_indexed, cluster_interpretations=best_interp)\nplt.suptitle('Pre√ßo com Regimes Otimizados pelo GA', y=1.02)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de barras: Probabilidade por Cluster\n",
    "clusters = []\n",
    "probs = []\n",
    "colors = []\n",
    "color_map = {'bull': 'green', 'bear': 'red', 'flat': 'gold'}\n",
    "\n",
    "for cluster_id, data in sorted(best_report['conditional_probabilities'].items()):\n",
    "    interp = best_interp.get(int(float(cluster_id)), 'flat')\n",
    "    clusters.append(f\"Cluster {int(float(cluster_id))}\\n({interp})\")\n",
    "    probs.append(data['probability_pct'])\n",
    "    colors.append(color_map.get(interp, 'gray'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(clusters, probs, color=colors, edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Linha horizontal para probabilidade incondicional\n",
    "ax.axhline(y=best_report['raw_probability_pct'], color='blue', linestyle='--', \n",
    "           linewidth=2, label=f\"P incondicional: {best_report['raw_probability_pct']:.1f}%\")\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, prob in zip(bars, probs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            f'{prob:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Probabilidade (%)')\n",
    "ax.set_title(f'Probabilidade de Atingir {ga_config.target_return:.1%} em {ga_config.horizon} dias por Cluster')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDelta P (separa√ß√£o): {best_report['separation_metrics']['delta_p']:.4f}\")\n",
    "print(f\"Melhor cluster: {max(best_report['conditional_probabilities'].items(), key=lambda x: x[1]['probability_pct'])[0]}\")\n",
    "print(f\"Pior cluster: {min(best_report['conditional_probabilities'].items(), key=lambda x: x[1]['probability_pct'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identificar regime atual e probabilidade\nlast_row = df_best.iloc[-1]\ncurrent_cluster = int(last_row['cluster'])\ncurrent_interp = best_interp.get(current_cluster, 'unknown')\ncurrent_prob = best_report['conditional_probabilities'][str(float(current_cluster))]['probability_pct']\n\nprint(\"=\" * 60)\nprint(\"SITUA√á√ÉO ATUAL\")\nprint(\"=\" * 60)\nprint(f\"\\nData mais recente: {last_row['date'].strftime('%Y-%m-%d')}\")\nprint(f\"Pre√ßo de fechamento: R$ {last_row['close']:.2f}\")\nprint(f\"\\nRegime atual: Cluster {current_cluster} ({current_interp.upper()})\")\n\n# Mostrar indicadores atuais\nprint(f\"\\n=== Indicadores Atuais ===\")\nprint(f\"  Slope ({best.window_slope}d): {last_row[f'slope_{best.window_slope}']:.6f}\")\nif best.use_volatility:\n    print(f\"  Volatility ({best.window_volatility}d): {last_row[f'volatility_{best.window_volatility}']:.4f}\")\nif best.use_rolling_return:\n    print(f\"  Log Return Rolling ({best.window_rolling_return}d): {last_row[f'log_return_rolling_{best.window_rolling_return}']:.4f}\")\nif best.use_ma_distance:\n    print(f\"  MA Distance ({best.ma_fast_period}/{best.ma_slow_period}): {last_row[f'ma_dist_{best.ma_fast_period}_{best.ma_slow_period}']:.4f}\")\nif best.use_trend_indicator:\n    trend_col = f'trend_indicator_norm_{best.window_trend_indicator}'\n    trend_value = last_row[trend_col]\n    trend_interpretation = \"BULLISH\" if trend_value > 0.3 else \"BEARISH\" if trend_value < -0.3 else \"NEUTRAL\"\n    print(f\"  Trend Indicator ({best.window_trend_indicator}d, mult={best.trend_slope_multiplier}): {trend_value:.4f} ({trend_interpretation})\")\n\nprint(f\"\\nProbabilidade de atingir {ga_config.target_return:.1%} em {ga_config.horizon} dias:\")\nprint(f\"  -> {current_prob:.1f}%\")\nprint(f\"\\nProbabilidade incondicional (m√©dia hist√≥rica): {best_report['raw_probability_pct']:.1f}%\")\n\n# Compara√ß√£o\ndiff = current_prob - best_report['raw_probability_pct']\nif diff > 0:\n    print(f\"\\n‚úÖ Regime atual tem probabilidade {diff:.1f} pontos percentuais ACIMA da m√©dia\")\nelse:\n    print(f\"\\n‚ö†Ô∏è Regime atual tem probabilidade {abs(diff):.1f} pontos percentuais ABAIXO da m√©dia\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## Etapa 15: An√°lise Dual - Fechar vs Tocar\n",
    "\n",
    "Comparamos duas probabilidades diferentes:\n",
    "- **P(fechar)**: Probabilidade do pre√ßo FECHAR acima do alvo em t+H\n",
    "- **P(tocar)**: Probabilidade do pre√ßo TOCAR o alvo em algum momento entre t+1 e t+H\n",
    "\n",
    "Esta an√°lise √© √∫til para opera√ß√µes de op√ß√µes e stop-loss/take-profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calculators import FutureTouchCalculatorVectorized\n",
    "from src.analysis import DualProbabilityCalculator\n",
    "\n",
    "# Adicionar colunas de touch ao DataFrame com par√¢metros otimizados\n",
    "touch_calc = FutureTouchCalculatorVectorized(horizon=ga_config.horizon)\n",
    "df_dual = touch_calc.calculate(df_best)\n",
    "\n",
    "print(\"=== Novas Colunas de Touch ===\")\n",
    "print(f\"Colunas adicionadas: {touch_calc.output_columns}\")\n",
    "print(f\"\\nExemplo dos dados:\")\n",
    "print(df_dual[['date', 'close', f'log_return_future_{ga_config.horizon}', \n",
    "               f'log_return_touch_max_{ga_config.horizon}']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar calculador dual\n",
    "dual_calc = DualProbabilityCalculator(\n",
    "    close_return_column=f'log_return_future_{ga_config.horizon}',\n",
    "    touch_return_column=f'log_return_touch_max_{ga_config.horizon}',  # Para alvos de alta\n",
    "    target_return=ga_config.target_return,\n",
    "    regime_column='cluster',\n",
    "    horizon=ga_config.horizon\n",
    ")\n",
    "\n",
    "# Imprimir compara√ß√£o formatada\n",
    "dual_calc.print_comparison(df_dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico comparativo: P(fechar) vs P(tocar) por cluster\n",
    "dual_report = dual_calc.generate_report(df_dual)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "clusters_list = sorted(dual_report['conditional_probabilities'].keys())\n",
    "x = np.arange(len(clusters_list))\n",
    "width = 0.35\n",
    "\n",
    "close_probs = [dual_report['conditional_probabilities'][c]['prob_close'] * 100 for c in clusters_list]\n",
    "touch_probs = [dual_report['conditional_probabilities'][c]['prob_touch'] * 100 for c in clusters_list]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, close_probs, width, label='P(fechar)', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, touch_probs, width, label='P(tocar)', color='coral', alpha=0.8)\n",
    "\n",
    "# Labels nos clusters\n",
    "cluster_labels = []\n",
    "for c in clusters_list:\n",
    "    interp = best_interp.get(int(float(c)), 'unknown')\n",
    "    cluster_labels.append(f\"Cluster {int(float(c))}\\n({interp})\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cluster_labels)\n",
    "ax.set_ylabel('Probabilidade (%)')\n",
    "ax.set_title(f'Compara√ß√£o: P(fechar) vs P(tocar) - Target: {ga_config.target_return:.1%} em {ga_config.horizon} dias')\n",
    "ax.legend()\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ratio touch/close\n",
    "print(\"\\n=== Ratio Touch/Close por Cluster ===\")\n",
    "for c in clusters_list:\n",
    "    data = dual_report['conditional_probabilities'][c]\n",
    "    ratio = data['touch_vs_close_ratio']\n",
    "    interp = best_interp.get(int(float(c)), 'unknown')\n",
    "    print(f\"Cluster {int(float(c))} ({interp}): {ratio:.2f}x mais prov√°vel tocar que fechar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Situa√ß√£o atual com an√°lise dual\nlast_row = df_dual.iloc[-1]\ncurrent_cluster = int(last_row['cluster'])\ncurrent_interp = best_interp.get(current_cluster, 'unknown')\n\ncurrent_data = dual_report['conditional_probabilities'][str(float(current_cluster))]\ncurrent_prob_close = current_data['prob_close'] * 100\ncurrent_prob_touch = current_data['prob_touch'] * 100\n\nprint(\"=\" * 60)\nprint(\"SITUA√á√ÉO ATUAL - AN√ÅLISE DUAL\")\nprint(\"=\" * 60)\nprint(f\"\\nData: {last_row['date'].strftime('%Y-%m-%d')}\")\nprint(f\"Pre√ßo: R$ {last_row['close']:.2f}\")\nprint(f\"Regime: Cluster {current_cluster} ({current_interp.upper()})\")\n\n# Mostrar indicadores atuais incluindo trend\nprint(f\"\\n=== Indicadores Atuais ===\")\nprint(f\"  Slope ({best.window_slope}d): {last_row[f'slope_{best.window_slope}']:.6f}\")\nif best.use_trend_indicator:\n    trend_col = f'trend_indicator_norm_{best.window_trend_indicator}'\n    trend_value = last_row[trend_col]\n    trend_interpretation = \"BULLISH\" if trend_value > 0.3 else \"BEARISH\" if trend_value < -0.3 else \"NEUTRAL\"\n    print(f\"  Trend Indicator ({best.window_trend_indicator}d, mult={best.trend_slope_multiplier}): {trend_value:.4f} ({trend_interpretation})\")\n\nprint(f\"\\nProbabilidade de atingir {ga_config.target_return:.1%} em {ga_config.horizon} dias:\")\nprint(f\"  P(fechar ‚â• alvo): {current_prob_close:.1f}%\")\nprint(f\"  P(tocar o alvo):  {current_prob_touch:.1f}%\")\nprint(f\"  Ratio:            {current_prob_touch/current_prob_close:.2f}x\")\n\nprint(f\"\\nüìà Interpreta√ß√£o:\")\nprint(f\"  √â {current_prob_touch/current_prob_close:.1f}x mais prov√°vel o pre√ßo TOCAR o alvo\")\nprint(f\"  do que FECHAR acima dele.\")\nprint(f\"\\n  Isso √© √∫til para:\")\nprint(f\"  - Op√ß√µes: P(tocar) relevante para barreiras knock-in/knock-out\")\nprint(f\"  - Trading: P(tocar) para stop-loss e take-profit\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 16: Salvando o Modelo para Produ√ß√£o\n",
    "\n",
    "Usamos `RegimePredictor` para encapsular o modelo treinado e facilitar previs√µes futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prediction import RegimePredictor\n",
    "\n",
    "# Criar predictor a partir dos resultados do GA\n",
    "predictor = RegimePredictor(\n",
    "    chromosome=best,                      # Cromossomo otimizado\n",
    "    target_return=ga_config.target_return,\n",
    "    horizon=ga_config.horizon,\n",
    "    ticker=ticker,\n",
    "    data_dir=\"../data\"\n",
    ")\n",
    "\n",
    "# Treinar com os dados hist√≥ricos\n",
    "predictor.fit(df)\n",
    "\n",
    "# Salvar modelo para uso futuro\n",
    "predictor.save(\"../models/bova11_predictor\")\n",
    "print(\"‚úÖ Modelo salvo em ../models/bova11_predictor.joblib e .json\")\n",
    "\n",
    "# Carregar modelo (simulando uso em produ√ß√£o)\n",
    "predictor_loaded = RegimePredictor.load(\"../models/bova11_predictor\", data_dir=\"../data\")\n",
    "print(\"‚úÖ Modelo carregado com sucesso!\")\n",
    "\n",
    "# Fazer previs√£o do regime atual\n",
    "result = predictor_loaded.predict_current()\n",
    "\n",
    "# Exibir status formatado\n",
    "predictor_loaded.print_status(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 17: Otimiza√ß√£o Multi-Alvo\n",
    "\n",
    "Para diferentes targets de retorno, o GA pode encontrar par√¢metros diferentes.\n",
    "O `MultiTargetOptimizer` roda GAs independentes para cada alvo e cria uma matriz de probabilidades.\n",
    "\n",
    "**NOTA**: Esta etapa √© MUITO intensiva computacionalmente. Use com cautela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exemplo de uso do StrikeGridOptimizer com Progress Callback e Cluster Explainer\n\nfrom src.optimization import StrikeGridOptimizer\nfrom src.optimization.progress_callback import SimpleProgressCallback\nfrom src.prediction import StrikeGridPredictor\nfrom src.analysis import CompositeExplainer, DecisionTreeExplainer\n\n# Pre√ßo atual do ativo\ncurrent_price = df['close'].iloc[-1]\nprint(f\"Pre√ßo atual: R$ {current_price:.2f}\")\n\n# Definir strikes (exemplo para BOVA11)\nstrikes = list(range(int(current_price * 0.95), int(current_price * 1.10), 1))  # -5% a +10%, step de R$ 1\nprint(f\"\\nStrikes definidos: {strikes}\")\nprint(f\"Total de GAs a executar: {len(strikes)}\")\n\n# Preview da convers√£o strike ‚Üí target\nprint(\"\\n=== Preview Strike ‚Üí Target ===\")\nprint(f\"{'Strike':<12} {'Retorno':<12} {'Dire√ß√£o':<10}\")\nprint(\"-\" * 34)\nfor strike in strikes[:5]:\n    target_return = (strike / current_price) - 1\n    direction = 'UP' if target_return > 0 else 'DOWN' if target_return < 0 else 'ATM'\n    print(f\"R$ {strike:<9.2f} {target_return*100:+.2f}%       {direction:<10}\")\nif len(strikes) > 5:\n    print(f\"... ({len(strikes) - 5} mais)\")\n\n# Usando multi_ga_config da c√©lula de configura√ß√£o centralizada\nprint(f\"\\nUsando multi_ga_config da c√©lula centralizada:\")\nprint(f\"  Popula√ß√£o: {multi_ga_config.population_size}\")\nprint(f\"  Gera√ß√µes: {multi_ga_config.generations}\")\nprint(f\"  Early Stopping: {multi_ga_config.early_stopping}\")\n\n# Factory para criar callback de progresso para cada GA\ndef create_ga_callback(strike_idx, strike):\n    return SimpleProgressCallback(\n        total_generations=multi_ga_config.generations,\n        print_every=5  # Atualiza a cada 5 gera√ß√µes\n    )\n\n# Executar otimiza√ß√£o\nstrike_optimizer = StrikeGridOptimizer(\n    df=df,\n    current_price=current_price,\n    strikes=strikes,\n    horizon=HORIZON,\n    ga_config=multi_ga_config\n)\n\nprint(f\"\\n‚è≥ Iniciando otimiza√ß√£o para {len(strikes)} strikes...\")\n\nstrike_results = strike_optimizer.run(\n    verbose=True,\n    ga_progress_callback_factory=create_ga_callback,\n    parallel_ga=False  # Melhor para Jupyter notebooks\n)\n\n# Salvar resultados\nstrike_results.save(\"../models/strike_grid_h2\")\n\n# ============================================================================\n# EXPLICA√á√ÉO DOS CLUSTERS POR STRIKE\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EXPLICA√á√ÉO DOS CLUSTERS POR STRIKE\")\nprint(\"=\" * 70)\n\n# Armazenar explica√ß√µes\nstrike_explanations = {}\n\nfor strike, result in strike_results.results.items():\n    print(f\"\\n--- Strike R$ {strike:.2f} ---\")\n    \n    # Criar pipeline e processar dados com os par√¢metros do strike\n    from src.optimization.calculator_factory import CalculatorFactory\n    \n    factory = CalculatorFactory(horizon=HORIZON)\n    strike_pipeline = factory.create_pipeline(result.chromosome)\n    feature_cols_strike = factory.get_feature_columns(result.chromosome)\n    \n    # Processar dados\n    df_strike = strike_pipeline.run(df)\n    \n    # Clustering\n    strike_kmeans = KMeansRegimeClassifier(\n        n_clusters=result.chromosome.n_clusters,\n        feature_columns=feature_cols_strike\n    )\n    df_strike = strike_kmeans.fit_predict(df_strike)\n    \n    # Explicar clusters\n    strike_explainer = DecisionTreeExplainer(max_depth=4, min_samples_leaf=20)\n    strike_explainer.fit(df_strike, cluster_column='cluster', feature_columns=feature_cols_strike)\n    \n    # M√©tricas\n    metrics = strike_explainer.get_metrics()\n    print(f\"  Accuracy: {metrics.cv_accuracy_mean:.1%} (+/- {metrics.cv_accuracy_std:.1%})\")\n    \n    # Top features\n    print(f\"  Top Features:\")\n    for feature, importance in strike_explainer.get_top_features(3):\n        print(f\"    - {feature}: {importance:.3f}\")\n    \n    # Guardar explica√ß√£o\n    strike_explanations[strike] = {\n        'explainer': strike_explainer,\n        'top_features': strike_explainer.get_top_features(3),\n        'accuracy': metrics.cv_accuracy_mean,\n        'rules': strike_explainer.get_rules()\n    }\n\n# Criar predictor\nstrike_predictor = StrikeGridPredictor.from_optimization(strike_results, df)\nstrike_predictor.save(\"../models/strike_grid_h2_predictor\")\n\n# Ver matriz de strikes\nstrike_predictor.print_matrix(ticker)"
  },
  {
   "cell_type": "code",
   "source": "# Resumo das explica√ß√µes por strike - Visualiza√ß√£o\nprint(\"=\" * 70)\nprint(\"RESUMO: FEATURES MAIS IMPORTANTES POR STRIKE\")\nprint(\"=\" * 70)\n\n# Criar DataFrame com resumo\nsummary_data = []\nfor strike, explanation in strike_explanations.items():\n    top_feat = explanation['top_features'][0] if explanation['top_features'] else ('N/A', 0)\n    summary_data.append({\n        'Strike': f\"R$ {strike:.0f}\",\n        'Target': f\"{((strike / current_price) - 1) * 100:+.1f}%\",\n        'Top Feature': top_feat[0],\n        'Importance': f\"{top_feat[1]:.3f}\",\n        'Accuracy': f\"{explanation['accuracy']:.1%}\"\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df.to_string(index=False))\n\n# Gr√°fico: Feature mais importante por strike\nfig, ax = plt.subplots(figsize=(14, 6))\n\nstrike_values = list(strike_explanations.keys())\ntop_features_by_strike = [exp['top_features'][0][0] if exp['top_features'] else 'N/A' \n                          for exp in strike_explanations.values()]\nimportances = [exp['top_features'][0][1] if exp['top_features'] else 0 \n               for exp in strike_explanations.values()]\n\n# Cores por feature\nunique_features = list(set(top_features_by_strike))\ncolor_palette = plt.cm.tab10.colors\nfeature_colors = {feat: color_palette[i % len(color_palette)] for i, feat in enumerate(unique_features)}\nbar_colors = [feature_colors[feat] for feat in top_features_by_strike]\n\nbars = ax.bar(range(len(strike_values)), importances, color=bar_colors, edgecolor='black', alpha=0.8)\n\nax.set_xticks(range(len(strike_values)))\nax.set_xticklabels([f\"R${s:.0f}\" for s in strike_values], rotation=45, ha='right')\nax.set_xlabel('Strike')\nax.set_ylabel('Importance')\nax.set_title('Feature Mais Importante por Strike')\n\n# Legenda\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=color, label=feat) for feat, color in feature_colors.items()]\nax.legend(handles=legend_elements, loc='upper right', fontsize=8)\n\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 18: Grade de Strikes para Op√ß√µes\n",
    "\n",
    "O `StrikeGridOptimizer` √© similar ao MultiTargetOptimizer, mas trabalha com pre√ßos absolutos (strikes).\n",
    "Ideal para precifica√ß√£o de op√ß√µes.\n",
    "\n",
    "**Fluxo:**\n",
    "1. Define strikes (ex: R$ 120, 125, 130...)\n",
    "2. Converte cada strike para target return: `(strike / pre√ßo_atual) - 1`\n",
    "3. Roda GA para cada target exato\n",
    "4. Gera matriz de probabilidades por strike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso do StrikeGridOptimizer com Progress Callback\n",
    "\n",
    "from src.optimization import StrikeGridOptimizer\n",
    "from src.optimization.progress_callback import SimpleProgressCallback\n",
    "from src.prediction import StrikeGridPredictor\n",
    "\n",
    "# Pre√ßo atual do ativo\n",
    "current_price = df['close'].iloc[-1]\n",
    "print(f\"Pre√ßo atual: R$ {current_price:.2f}\")\n",
    "\n",
    "# Definir strikes (exemplo para BOVA11)\n",
    "strikes = list(range(int(current_price * 0.95), int(current_price * 1.10), 1))  # -5% a +10%, step de R$ 1\n",
    "print(f\"\\nStrikes definidos: {strikes}\")\n",
    "print(f\"Total de GAs a executar: {len(strikes)}\")\n",
    "\n",
    "# Preview da convers√£o strike ‚Üí target\n",
    "print(\"\\n=== Preview Strike ‚Üí Target ===\")\n",
    "print(f\"{'Strike':<12} {'Retorno':<12} {'Dire√ß√£o':<10}\")\n",
    "print(\"-\" * 34)\n",
    "for strike in strikes[:5]:\n",
    "    target_return = (strike / current_price) - 1\n",
    "    direction = 'UP' if target_return > 0 else 'DOWN' if target_return < 0 else 'ATM'\n",
    "    print(f\"R$ {strike:<9.2f} {target_return*100:+.2f}%       {direction:<10}\")\n",
    "if len(strikes) > 5:\n",
    "    print(f\"... ({len(strikes) - 5} mais)\")\n",
    "\n",
    "# Usando multi_ga_config da c√©lula de configura√ß√£o centralizada\n",
    "print(f\"\\nUsando multi_ga_config da c√©lula centralizada:\")\n",
    "print(f\"  Popula√ß√£o: {multi_ga_config.population_size}\")\n",
    "print(f\"  Gera√ß√µes: {multi_ga_config.generations}\")\n",
    "print(f\"  Early Stopping: {multi_ga_config.early_stopping}\")\n",
    "\n",
    "# Factory para criar callback de progresso para cada GA\n",
    "def create_ga_callback(strike_idx, strike):\n",
    "    return SimpleProgressCallback(\n",
    "        total_generations=multi_ga_config.generations,\n",
    "        print_every=5  # Atualiza a cada 5 gera√ß√µes\n",
    "    )\n",
    "\n",
    "# Executar otimiza√ß√£o\n",
    "strike_optimizer = StrikeGridOptimizer(\n",
    "    df=df,\n",
    "    current_price=current_price,\n",
    "    strikes=strikes,\n",
    "    horizon=HORIZON,\n",
    "    ga_config=multi_ga_config\n",
    ")\n",
    "\n",
    "print(f\"\\n‚è≥ Iniciando otimiza√ß√£o para {len(strikes)} strikes...\")\n",
    "\n",
    "strike_results = strike_optimizer.run(\n",
    "    verbose=True,\n",
    "    ga_progress_callback_factory=create_ga_callback,\n",
    "    parallel_ga=False  # Melhor para Jupyter notebooks\n",
    ")\n",
    "\n",
    "# Salvar resultados\n",
    "strike_results.save(\"../models/strike_grid_h2\")\n",
    "\n",
    "# Criar predictor\n",
    "strike_predictor = StrikeGridPredictor.from_optimization(strike_results, df)\n",
    "strike_predictor.save(\"../models/strike_grid_h2_predictor\")\n",
    "\n",
    "# Ver matriz de strikes\n",
    "strike_predictor.print_matrix(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclus√£o\n",
    "\n",
    "Este notebook demonstrou o fluxo completo do sistema QuantNote:\n",
    "\n",
    "1. **Configura√ß√£o** com valida√ß√£o via Pydantic\n",
    "2. **Obten√ß√£o de dados** do Yahoo Finance com rate limiting\n",
    "3. **Valida√ß√£o** de dados com m√∫ltiplos validators\n",
    "4. **Pipeline de calculadores** com resolu√ß√£o autom√°tica de depend√™ncias\n",
    "5. **Classifica√ß√£o de regimes** (manual e K-Means)\n",
    "6. **C√°lculo de probabilidades** condicionais\n",
    "7. **Visualiza√ß√£o** de distribui√ß√µes\n",
    "8. **Walk-forward validation** para evitar overfitting\n",
    "9. **Otimiza√ß√£o** com algoritmo gen√©tico\n",
    "10. **An√°lise dual** - P(fechar) vs P(tocar)\n",
    "\n",
    "### M√©tricas de Probabilidade\n",
    "\n",
    "- **P(fechar)**: Probabilidade de fechar acima do alvo no final do per√≠odo\n",
    "- **P(tocar)**: Probabilidade de atingir o alvo em algum momento durante o per√≠odo\n",
    "\n",
    "### Pr√≥ximos Passos\n",
    "\n",
    "- Testar com outros ativos\n",
    "- Ajustar par√¢metros do GA para busca mais ampla\n",
    "- Implementar novos indicadores (RSI, MACD, etc.)\n",
    "- Integrar com sistema de backtesting\n",
    "- Usar P(tocar) para otimiza√ß√£o de op√ß√µes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}